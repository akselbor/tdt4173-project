{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from sklearn import datasets\n",
    "from sklearn import impute\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import compose\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "warnings.filterwarnings('ignore')\n",
    "# pip install gspread and df2gspread\n",
    "import sys\n",
    "!{sys.executable} -m pip install gspread\n",
    "!{sys.executable} -m pip install numpy df2gspread\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California housing data set.\n",
    "# Modified to a classification problem. We want to predict whether a house costs more or less\n",
    "# than the median price.\n",
    "X0, Y0 = datasets.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X0 = preprocessing.StandardScaler().fit_transform(X0)\n",
    "Y0 = 2 * (Y0 >= np.median(Y0)).astype(np.float32) - 1\n",
    "\n",
    "X0.shape, Y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Adult' data set from UCI repository\n",
    "path = 'data/adult/adult'\n",
    "adult_data = pd.read_csv(f'{path}.data', header=None)\n",
    "adult_test = pd.read_csv(f'{path}.test', skiprows=1, header=None)\n",
    "adult = pd.concat([adult_data, adult_test])\n",
    "# These are the columns containing categorical values\n",
    "categorical = [13, 9, 8, 7, 6, 5, 3, 1]\n",
    "\n",
    "X1 = Pipeline([\n",
    "    ('impute_missing', impute.SimpleImputer(missing_values=' ?', strategy='most_frequent')),\n",
    "    ('norm_and_onehot',\n",
    "        compose.ColumnTransformer(\n",
    "            sparse_threshold=0, \n",
    "            transformers=[\n",
    "                ('onehot', preprocessing.OneHotEncoder(), categorical)\n",
    "            ], \n",
    "            remainder=preprocessing.StandardScaler()\n",
    "        )\n",
    "    ),\n",
    "    ('pca', PCA(n_components=0.8, svd_solver='full')),\n",
    "]).fit_transform(adult.drop(14, axis=1).to_numpy())\n",
    "\n",
    "Y1 = adult[14].to_numpy()\n",
    "# A boolean array using +1 for >50K and -1 for <=50K.\n",
    "Y1 = 2 * np.logical_or(Y1 == ' >50K.', Y1 == ' >50K').astype(np.float) - 1\n",
    "\n",
    "X1.shape, Y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/optdigits/optdigits.tra'\n",
    "orig_train = pd.read_csv(f'data/optdigits/optdigits.tra', header=None)\n",
    "orig_test = pd.read_csv(f'data/optdigits/optdigits.tes', header=None)\n",
    "optdigits = pd.concat([orig_train, orig_test])\n",
    "\n",
    "X2 = preprocessing.StandardScaler().fit_transform(optdigits.to_numpy()[:, :-1])\n",
    "Y2 = optdigits.to_numpy()[:, -1]\n",
    "\n",
    "X2.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'data/abalone/abalone.data'\n",
    "abalone = pd.read_csv(path, header=None).to_numpy()\n",
    "\n",
    "X3 = Pipeline([\n",
    "    ('preprocess', \n",
    "         compose.ColumnTransformer(\n",
    "            sparse_threshold=0,\n",
    "            transformers=[\n",
    "                ('onehot', preprocessing.OneHotEncoder(), [0])\n",
    "            ],\n",
    "            remainder=preprocessing.StandardScaler())\n",
    "    ),\n",
    "]).fit_transform(abalone[:, :-1])\n",
    "\n",
    "Y3 = 2 * (abalone[:, -1] <= 9).astype(np.float) - 1\n",
    "\n",
    "X3.shape, Y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    (X0, Y0, 'california_housing'),\n",
    "    (X1, Y1, 'adult'),\n",
    "    (X2, Y2, 'optdigits'),\n",
    "    (X3, Y3, 'abalone'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name, estimator, param_grid to be used for grid search,\n",
    "CLASSIFIERS = [\n",
    "    # Random forest.\n",
    "    (\n",
    "        'RF',\n",
    "        RandomForestClassifier(), \n",
    "        {\n",
    "            'max_depth': [2, 4, 8, 16], \n",
    "            'n_estimators': [50, 100, 150]\n",
    "        },\n",
    "    ),\n",
    "    # k-NN.\n",
    "    (\n",
    "        'KNN',\n",
    "        neighbors.KNeighborsClassifier(), \n",
    "        {\n",
    "            'n_neighbors': [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "        },\n",
    "    ),\n",
    "    # SVM.\n",
    "    (\n",
    "        'SVM',\n",
    "        svm.LinearSVC(penalty='l2', loss='hinge', max_iter=10000),\n",
    "        {\n",
    "            'C': [10**r for r in range(-8, 5)],\n",
    "        },\n",
    "    ),\n",
    "    # DT\n",
    "    (\n",
    "        'DT',\n",
    "        tree.DecisionTreeClassifier(min_samples_split=0.05),\n",
    "        {\n",
    "            'min_samples_split': [0.025, 0.05, 0.1, 0.2],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "    ),\n",
    "    # LR\n",
    "    (\n",
    "        'LR',\n",
    "        linear_model.LogisticRegression(),\n",
    "        {\n",
    "            'C': [10**r for r in range(-8, 5)],\n",
    "        },\n",
    "    ),\n",
    "    # ANN\n",
    "    (\n",
    "        'ANN',\n",
    "        neural_network.MLPClassifier(),\n",
    "        {\n",
    "            'alpha': [10**r for r in range(-5, 0)],\n",
    "            'activation': ['relu', 'logistic', 'tanh'],\n",
    "            'hidden_layer_sizes': [(64,), (72, 24), (16, 13, 7),],\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Just while testing\n",
    "CLASSIFIERS = [    \n",
    "    (\n",
    "        'DT',\n",
    "        tree.DecisionTreeClassifier(min_samples_split=0.05),\n",
    "        {\n",
    "            'min_samples_split': [0.025, 0.05],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For i in three different datasets\n",
    "   - For j in three different partitions (20/80,50/50,80/20):\n",
    "        - For t in three different trials\n",
    "            - For c in three different classifiers\n",
    "                 - cross validate\n",
    "                 - find the optimal hyper-parameter\n",
    "                 - train using the hyper-parameter above\n",
    "                 - obtain the training and validation accuracy/error\n",
    "                 - test\n",
    "                 - obtain the testing accuracy\n",
    "       - compute the averaged accuracy (training, validation, and testing) for each classifier c out of three trials\n",
    "       - rank order the classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 datasets x 3 partitions x 3 trials x 3 classifiers.\n",
    "def params():\n",
    "    # This is gonna take some time.\n",
    "    for (X, Y, name) in DATASETS:\n",
    "        for p in [0.2, 0.5, 0.8]:\n",
    "            for trial in range(3):\n",
    "                # The trial data.\n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = p)\n",
    "                # Run this trial over each classifier.\n",
    "                for (clf_name, clf, params) in CLASSIFIERS:\n",
    "                    yield {\n",
    "                        # Meta data to be used for data collection purposes.\n",
    "                        'meta': {\n",
    "                            'dataset': name,\n",
    "                            'train_size': p,\n",
    "                            'trial': trial,\n",
    "                            'classifier': clf_name,\n",
    "                        },\n",
    "                        # Used in order to train this model.\n",
    "                        'data': {\n",
    "                            'clf': clf,\n",
    "                            'params': params,\n",
    "                            'x_train': X_train,\n",
    "                            'y_train': Y_train,\n",
    "                            'x_test': X_test,\n",
    "                            'y_test': Y_test,\n",
    "                        }\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_instance(x_train, y_train, x_test, y_test, clf, params):\n",
    "    # Fit the classifier to the transformed input.\n",
    "    grid_search = GridSearchCV(clf, params, cv=5, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    # After determining the best hyper parameters, we can attain the\n",
    "    # estimator that performed the best.\n",
    "    estimator = grid_search.best_estimator_\n",
    "    \n",
    "    # Then, we need these four metrics from the trained classifier\n",
    "    train = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]\n",
    "    validation = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]\n",
    "    test = estimator.score(x_test, y_test)\n",
    "    # As well as the actual parameters that were used\n",
    "    params = grid_search.best_params_\n",
    "    \n",
    "    # This is used to compute f-score\n",
    "    y_pred = estimator.predict(x_test)\n",
    "    \n",
    "    return {\n",
    "        'f_score': f1_score(y_test, y_pred, average='macro'),\n",
    "        'training_accuracy': train,\n",
    "        'validation_accuracy': validation,\n",
    "        'test_accuracy': test,\n",
    "        'params': params,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "    }\n",
    "    \n",
    "def train_instance(param):\n",
    "    return {\n",
    "        **param['meta'],\n",
    "        **_train_instance(**param['data']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seed = 1, parameters = params()):\n",
    "    # For reproducible results\n",
    "    np.random.seed(seed)\n",
    "    # Parallel training\n",
    "    return pd.DataFrame(\n",
    "        Parallel(n_jobs=-1, verbose=50)(\n",
    "            delayed(train_instance)(param) for param in parameters\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "Since training is boring and time consuming, we can attempt to parallelize it as much as possible. The `train` method above is already written to utilize multiple threads. However, we can do better. We can distribute the computation among several workers, enabling a substantial speed up. Therefore, I am proud to present the world's most ad hoc distributed machine learning training algorithm, ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# When distributing work across several machines, we'll use a very simple (and inefficient) method to divide\n",
    "# the work between them. We'll simply assign each of N machines an id in range(0, N), and give each of them an\n",
    "# equal chunk of the total work. The id is here represented by the environment variable `WORKER_NUM`, while the\n",
    "# number of workers is denoted by `TOTAL_WORKERS`. Default: *single machine doing all the work*.\n",
    "user = int(os.environ.get('WORKER_NUM', 0))\n",
    "workers = int(os.environ.get('TOTAL_WORKERS', 1))\n",
    "\n",
    "# Retrieve the work to be done by this instance.\n",
    "work = list(params())\n",
    "items_per_worker = len(work) // workers + 1\n",
    "work = work[user * items_per_worker: (user + 1) * items_per_worker]\n",
    "\n",
    "# Train on this worker's subset.\n",
    "results = train(seed = user, parameters=work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to print the results if running with in Jupyter with a GUI.\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"This is just here to stop the upload to google sheets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Google Sheets\n",
    "Now after computing the results, we'll want to upload it to Google Sheets. This enables us to easily crowdsource the computation between gutta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: The keyfile is not included in this submission, so this portion of the project will crash horribly. But, it's only really needed when doing distributed training anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorization for using Google Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we want the credentials to be sent in order to be authorized\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "keyfile = 'gserviceaccount-client-secret.json'\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(keyfile, scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1QUMP6tlBqR3CqYlh7uC18e_XA4FgMT9SqMSc5agoGUA'\n",
    "\n",
    "# This adjusts between our 0-indexing and GS' 1-indexing, as well as the extra space\n",
    "# taken up by the header row.\n",
    "offset = 1 if user == 0 else 2\n",
    "d2g.upload(\n",
    "    results,\n",
    "    spreadsheet_key,\n",
    "    credentials=credentials,\n",
    "    clean=False,\n",
    "    row_names=False,\n",
    "    col_names=user == 0,\n",
    "    start_cell=f'A{user * items_per_worker + offset}',\n",
    "    wks_name = 'Results',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
