{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import warnings\n",
    "import zipfile\n",
    "from joblib import Parallel, delayed\n",
    "from pandas import CategoricalDtype\n",
    "import sklearn\n",
    "from sklearn import datasets, impute, neighbors, preprocessing, svm, compose, linear_model, neural_network, tree\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pip install and import gspread and df2gspread, as they're used to upload to Google Sheets\n",
    "warnings.filterwarnings('ignore')\n",
    "!{sys.executable} -m pip install gspread\n",
    "!{sys.executable} -m pip install numpy df2gspread\n",
    "import gspread\n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators \n",
    "\n",
    "These estimators are inspired by the estimators found in this article:  https://towardsdatascience.com/logistic-regression-classifier-on-census-income-data-e1dbef0b5738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for column in self.categories:\n",
    "            X_copy[column] = X_copy[column].astype('object')\n",
    "        return pd.DataFrame(X_copy)\n",
    "\n",
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, types):\n",
    "        self.type = types\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.select_dtypes(include=self.type)\n",
    "\n",
    "\n",
    "class CategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, strategy='most_frequent', columns=None):\n",
    "        self.strategy = strategy\n",
    "        self.columns = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns is None:\n",
    "            self.columns = X.columns\n",
    "        \n",
    "        if self.strategy is 'most_frequent':\n",
    "            # X[column].value_counts() returns pandas dataseries with index equal to the \n",
    "            # elements that are counted, and columns equal to the columns in X. The values are \n",
    "            # the count for each index. \n",
    "            # index[0] returns the first index, which is the index with the highest count\n",
    "            self.fill = {column:X[column].value_counts().index[0] for column in self.columns}\n",
    "            \n",
    "        else:\n",
    "            {column:0 for column in self.columns}  # If there is another strategy just replace missing values with 0.\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for column in self.columns:\n",
    "            X_copy[column] = X_copy[column].fillna(self.fill[column])\n",
    "        return X_copy\n",
    "    \n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, drop_first=True): # Set drop first in order to deal with multicollinearity\n",
    "        self.drop_first = drop_first\n",
    "        self.categories = dict()  # Categories to encode\n",
    "    \n",
    "    def fit(self, X, y=None):  # important that X is the entire data set, so we find all categories\n",
    "        X = X.select_dtypes(include=['object'])\n",
    "        for column in X.columns:\n",
    "            self.categories[column] = X[column].value_counts().index.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = X_copy.select_dtypes(include=['object'])\n",
    "        for column in X_copy.columns:\n",
    "            #\"Add\" all possible attributes to the column such that every category is included in the encoding\n",
    "            X_copy[column] = X_copy[column].astype({column:CategoricalDtype(self.categories[column])})\n",
    "        return pd.get_dummies(X_copy, drop_first=self.drop_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used for downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(path, url):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    filename = os.path.basename(url)\n",
    "    if not os.path.exists(os.path.join(path,filename)): # Only download data if it doesn't already exists\n",
    "        response = requests.get(url)\n",
    "        with open(os.path.join(path,filename), 'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California housing data set.\n",
    "# Modified to a classification problem. We want to predict whether a house costs more or less\n",
    "# than the median price.\n",
    "X0, Y0 = datasets.fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X0 = preprocessing.StandardScaler().fit_transform(X0)\n",
    "Y0 = 2 * (Y0 >= np.median(Y0)).astype(np.float32) - 1\n",
    "\n",
    "X0.shape, Y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Adult' data set from UCI repository\n",
    "path = 'data/adult/adult'\n",
    "adult_data = pd.read_csv(f'{path}.data', header=None)\n",
    "adult_test = pd.read_csv(f'{path}.test', skiprows=1, header=None)\n",
    "adult = pd.concat([adult_data, adult_test])\n",
    "# These are the columns containing categorical values\n",
    "categorical = [13, 9, 8, 7, 6, 5, 3, 1]\n",
    "\n",
    "X1 = Pipeline([\n",
    "    ('impute_missing', impute.SimpleImputer(missing_values=' ?', strategy='most_frequent')),\n",
    "    ('norm_and_onehot',\n",
    "        compose.ColumnTransformer(\n",
    "            sparse_threshold=0, \n",
    "            transformers=[\n",
    "                ('onehot', preprocessing.OneHotEncoder(), categorical)\n",
    "            ], \n",
    "            remainder=preprocessing.StandardScaler()\n",
    "        )\n",
    "    ),\n",
    "    ('pca', PCA(n_components=0.8, svd_solver='full')),\n",
    "]).fit_transform(adult.drop(14, axis=1).to_numpy())\n",
    "\n",
    "Y1 = adult[14].to_numpy()\n",
    "# A boolean array using +1 for >50K and -1 for <=50K.\n",
    "Y1 = 2 * np.logical_or(Y1 == ' >50K.', Y1 == ' >50K').astype(np.float) - 1\n",
    "\n",
    "X1.shape, Y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/optdigits/optdigits.tra'\n",
    "orig_train = pd.read_csv(f'data/optdigits/optdigits.tra', header=None)\n",
    "orig_test = pd.read_csv(f'data/optdigits/optdigits.tes', header=None)\n",
    "optdigits = pd.concat([orig_train, orig_test])\n",
    "\n",
    "X2 = preprocessing.StandardScaler().fit_transform(optdigits.to_numpy()[:, :-1])\n",
    "Y2 = optdigits.to_numpy()[:, -1]\n",
    "\n",
    "X2.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'data/abalone/abalone.data'\n",
    "abalone = pd.read_csv(path, header=None).to_numpy()\n",
    "\n",
    "X3 = Pipeline([\n",
    "    ('preprocess', \n",
    "         compose.ColumnTransformer(\n",
    "            sparse_threshold=0,\n",
    "            transformers=[\n",
    "                ('onehot', preprocessing.OneHotEncoder(), [0])\n",
    "            ],\n",
    "            remainder=preprocessing.StandardScaler())\n",
    "    ),\n",
    "]).fit_transform(abalone[:, :-1])\n",
    "\n",
    "Y3 = 2 * (abalone[:, -1] <= 9).astype(np.float) - 1\n",
    "\n",
    "X3.shape, Y3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Shoppers Intention Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_url = ('https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv')\n",
    "\n",
    "download(folder_name, online_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load online shoppers intention data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv(os.path.join(folder_name, os.path.basename(online_url)), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_copy = online.copy()\n",
    "online_copy['Revenue'] = online_copy['Revenue'].map({True:1,False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_online = online_copy.drop('Revenue', axis=1)\n",
    "Y_online = online_copy['Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['SpecialDay','Month','OperatingSystems', 'Browser', 'Region','TrafficType', 'VisitorType', 'Weekend']\n",
    "\n",
    "online_full_pipe = FeatureUnion(\n",
    "    [\n",
    "        (\n",
    "            'cat_pipe', \n",
    "            Pipeline(\n",
    "                [\n",
    "                    ('Converter', Converter(categories=categories)),\n",
    "                    ('Selector', ColumnsSelector(types=['object'])),\n",
    "                    ('Encoder', CategoricalEncoder())\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'num_pipe', \n",
    "            Pipeline(\n",
    "                [\n",
    "                    ('Converter', Converter(categories=categories)),\n",
    "                    ('Selector', ColumnsSelector(types=['int','float'])),\n",
    "                    ('Scaler', StandardScaler()),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "online_pca_pipe = Pipeline([('online_pipeline', online_full_pipe), ('PCA', PCA(n_components=0.8))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Marketing Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip'\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_name, os.path.basename(bank_url))):\n",
    "    response = requests.get(bank_url)\n",
    "    with open(os.path.join(folder_name, os.path.basename(bank_url)),'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    with zipfile.ZipFile(os.path.join(folder_name, os.path.basename(bank_url)), 'r') as f:\n",
    "        print(os.path.join(folder_name, os.path.basename(bank_url)))\n",
    "        f.extractall(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_full_pipeline = FeatureUnion(\n",
    "    [\n",
    "        (\n",
    "            'num_pipeline', \n",
    "            Pipeline(\n",
    "                [\n",
    "                    ('num_attri_selector', ColumnsSelector(types=['int'])),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        (\n",
    "            'cat_pipeline', \n",
    "            Pipeline(\n",
    "                [\n",
    "                    ('cat_attri_selector', ColumnsSelector(types=['object'])),\n",
    "                    ('encoder', CategoricalEncoder()),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bank_pca_pipe = Pipeline([('bank_pipeline', bank_full_pipeline), ('PCA', PCA(n_components=0.8))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = pd.read_csv(os.path.join(folder_name,'bank-full.csv'), sep=';')\n",
    "\n",
    "bank_copy = bank.copy() # Copy the train data to keep the original as it is\n",
    "bank_copy['y'] = bank_copy['y'].map({'yes':1,'no':0})\n",
    "X_bank = bank_copy.drop('y', axis=1)\n",
    "Y_bank = bank_copy['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset declaration\n",
    "This cell declares the data sets that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    (X0, Y0, 'california_housing'),\n",
    "    (X1, Y1, 'adult'),\n",
    "    (X2, Y2, 'optdigits'),\n",
    "    (X3, Y3, 'abalone'),\n",
    "    (online_full_pipe.fit_transform(X_online), Y_online, 'online'),\n",
    "    (online_pca_pipe.fit_transform(X_online), Y_online, 'online (pca)'),\n",
    "    (bank_full_pipeline.fit_transform(X_bank), Y_bank, 'bank'),\n",
    "    (bank_pca_pipe.fit_transform(X_bank), Y_bank, 'bank (pca)'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name, estimator, param_grid to be used for grid search,\n",
    "CLASSIFIERS = [\n",
    "    # Random forest.\n",
    "    (\n",
    "        'RF',\n",
    "        RandomForestClassifier(), \n",
    "        {\n",
    "            'max_depth': [2, 4, 8, 16], \n",
    "            'n_estimators': [50, 100, 150]\n",
    "        },\n",
    "    ),\n",
    "    # k-NN.\n",
    "    (\n",
    "        'KNN',\n",
    "        neighbors.KNeighborsClassifier(), \n",
    "        {\n",
    "            'n_neighbors': [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "        },\n",
    "    ),\n",
    "    # SVM.\n",
    "    (\n",
    "        'SVM',\n",
    "        svm.LinearSVC(penalty='l2', loss='hinge', max_iter=10000),\n",
    "        {\n",
    "            'C': [10**r for r in range(-8, 5)],\n",
    "        },\n",
    "    ),\n",
    "    # DT\n",
    "    (\n",
    "        'DT',\n",
    "        tree.DecisionTreeClassifier(min_samples_split=0.05),\n",
    "        {\n",
    "            'min_samples_split': [0.025, 0.05, 0.1, 0.2],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "    ),\n",
    "    # LR\n",
    "    (\n",
    "        'LR',\n",
    "        linear_model.LogisticRegression(),\n",
    "        {\n",
    "            'C': [10**r for r in range(-8, 5)],\n",
    "        },\n",
    "    ),\n",
    "    # ANN\n",
    "    (\n",
    "        'ANN',\n",
    "        neural_network.MLPClassifier(),\n",
    "        {\n",
    "            'alpha': [10**r for r in range(-5, 0)],\n",
    "            'activation': ['relu', 'logistic', 'tanh'],\n",
    "            'hidden_layer_sizes': [(64,), (72, 24), (16, 13, 7),],\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 datasets x 3 partitions x 3 trials x 3 classifiers.\n",
    "def params():\n",
    "    # This is gonna take some time.\n",
    "    for (X, Y, name) in DATASETS:\n",
    "        for p in [0.2, 0.5, 0.8]:\n",
    "            for trial in range(3):\n",
    "                # The trial data.\n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = p)\n",
    "                # Run this trial over each classifier.\n",
    "                for (clf_name, clf, params) in CLASSIFIERS:\n",
    "                    yield {\n",
    "                        # Meta data to be used for data collection purposes.\n",
    "                        'meta': {\n",
    "                            'dataset': name,\n",
    "                            'train_size': p,\n",
    "                            'trial': trial,\n",
    "                            'classifier': clf_name,\n",
    "                        },\n",
    "                        # Used in order to train this model.\n",
    "                        'data': {\n",
    "                            'clf': clf,\n",
    "                            'params': params,\n",
    "                            'x_train': X_train,\n",
    "                            'y_train': Y_train,\n",
    "                            'x_test': X_test,\n",
    "                            'y_test': Y_test,\n",
    "                        }\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_instance(x_train, y_train, x_test, y_test, clf, params):\n",
    "    # Fit the classifier to the transformed input.\n",
    "    grid_search = GridSearchCV(clf, params, cv=5, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    # After determining the best hyper parameters, we can attain the\n",
    "    # estimator that performed the best.\n",
    "    estimator = grid_search.best_estimator_\n",
    "    \n",
    "    # Then, we need these four metrics from the trained classifier\n",
    "    train = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]\n",
    "    validation = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]\n",
    "    test = estimator.score(x_test, y_test)\n",
    "    # As well as the actual parameters that were used\n",
    "    params = grid_search.best_params_\n",
    "    \n",
    "    # This is used to compute f-score\n",
    "    y_pred = estimator.predict(x_test)\n",
    "    \n",
    "    return {\n",
    "        'f_score': f1_score(y_test, y_pred, average='macro'),\n",
    "        'training_accuracy': train,\n",
    "        'validation_accuracy': validation,\n",
    "        'test_accuracy': test,\n",
    "        'params': params,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "    }\n",
    "    \n",
    "def train_instance(param):\n",
    "    return {\n",
    "        **param['meta'],\n",
    "        **_train_instance(**param['data']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seed = 1, parameters = params()):\n",
    "    # For reproducible results\n",
    "    np.random.seed(seed)\n",
    "    # Parallel training\n",
    "    return pd.DataFrame(\n",
    "        Parallel(n_jobs=-1, verbose=50)(\n",
    "            delayed(train_instance)(param) for param in parameters\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "Since training is boring and time consuming, we can attempt to parallelize it as much as possible. The `train` method above is already written to utilize multiple threads. However, we can do better. We can distribute the computation among several workers, enabling a substantial speed up. Therefore, I am proud to present the world's most ad hoc distributed machine learning training algorithm, ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# When distributing work across several machines, we'll use a very simple (and inefficient) method to divide\n",
    "# the work between them. We'll simply assign each of N machines an id in range(0, N), and give each of them an\n",
    "# equal chunk of the total work. The id is here represented by the environment variable `WORKER_NUM`, while the\n",
    "# number of workers is denoted by `TOTAL_WORKERS`. Default: *single machine doing all the work*.\n",
    "user = int(os.environ.get('WORKER_NUM', 0))\n",
    "workers = int(os.environ.get('TOTAL_WORKERS', 1))\n",
    "\n",
    "# Retrieve the work to be done by this instance.\n",
    "work = list(params())\n",
    "items_per_worker = len(work) // workers + 1\n",
    "work = work[user * items_per_worker: (user + 1) * items_per_worker]\n",
    "\n",
    "# Train on this worker's subset.\n",
    "results = train(seed = user, parameters=work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to print the results if running with in Jupyter with a GUI.\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Google Sheets\n",
    "Now after computing the results, we'll want to upload it to Google Sheets. This enables us to easily crowdsource the computation between gutta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: The keyfile is not included in this submission, so this portion of the project will crash horribly. But, it's only really needed when doing distributed training anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorization for using Google Cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we want the credentials to be sent in order to be authorized\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "keyfile = 'gserviceaccount-client-secret.json'\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(keyfile, scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1QUMP6tlBqR3CqYlh7uC18e_XA4FgMT9SqMSc5agoGUA'\n",
    "\n",
    "# This adjusts between our 0-indexing and GS' 1-indexing, as well as the extra space\n",
    "# taken up by the header row.\n",
    "offset = 1 if user == 0 else 2\n",
    "d2g.upload(\n",
    "    results,\n",
    "    spreadsheet_key,\n",
    "    credentials=credentials,\n",
    "    clean=False,\n",
    "    row_names=False,\n",
    "    col_names=user == 0,\n",
    "    start_cell=f'A{user * items_per_worker + offset}',\n",
    "    wks_name = 'Results',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
